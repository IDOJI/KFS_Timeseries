}
}
}
}
}
} else {
cat("\n", crayon::red("Data copy cancelled."), "\n")
}
}
}
# 🟧 모든 연도 합치는 함수 (최빈값 기준) ================================================================
process_and_export_most_frequent_value_rows <- function(path_to, output_file) {
# 지정된 경로에서 파일 리스트 가져오기
files <- list.files(path_to, full.names = TRUE)
# "연도.csv" 형태의 파일만 필터링
csv_files <- files[grepl("_[0-9]{4}\\.csv$", files)]
# 파일을 연도 순서대로 정렬
csv_files <- csv_files[order(as.numeric(gsub(".*_([0-9]{4})\\.csv$", "\\1", csv_files)))]
# 파일이 하나만 존재하는 경우 함수 실행 생략
if (length(csv_files) <= 1) {
cat("\n", crayon::red("파일이 하나만 존재하거나 없습니다. 함수 실행을 생략합니다."), "\n")
return(NULL)
}
# 각 파일을 읽고 Value_new의 최빈값 행만 추출
most_frequent_value_rows <- lapply(csv_files, function(file) {
df <- read_csv(file, show_col_types = FALSE)
# 최빈값 계산
most_frequent_value <- as.numeric(names(sort(table(df$Value_new), decreasing = TRUE)[1]))
most_frequent_rows <- df %>% filter(Value_new == most_frequent_value)
# Classification 열을 기준으로 가장 오래된 연도 선택
selected_row <- most_frequent_rows %>% arrange(Sub_Sub_Category) %>% slice(1)
# Sub_Sub_Category 열을 character로 변환
selected_row <- selected_row %>% mutate(Sub_Sub_Category = as.character(Sub_Sub_Category))
return(selected_row)
})
# 모든 추출한 행들을 하나의 데이터프레임으로 합치기
combined_df <- bind_rows(most_frequent_value_rows)
# Sub_Sub_Category와 Value_new가 제대로 읽히도록 형 변환
combined_df <- combined_df %>%
mutate(Sub_Sub_Category = as.factor(Sub_Sub_Category), Value_new = as.numeric(Value_new))
# View(combined_df)
# 연도 순서 확인
years = combined_df$Sub_Sub_Category %>% gsub("까지", "", .) %>% as.numeric %>% sort
all_years_present <- all(diff(years) == 1)
first_year <- min(years)
last_year <- max(years)
if (all_years_present) {
output_file <- paste0(output_file, "_complete")
message <- sprintf("모든 연도가 1년 차이로 존재합니다. (%d년부터 %d년까지)", first_year, last_year)
} else {
output_file <- paste0(output_file, "_incomplete")
message <- sprintf("연도 사이에 누락된 연도가 있습니다. (%d년부터 %d년까지)", first_year, last_year)
}
# 지정한 파일 이름으로 데이터프레임 내보내기
write_csv(combined_df, file.path(path_to, paste0(output_file, ".csv")))
# ggplot을 이용한 timeseries plot 생성
p <- ggplot(combined_df, aes(x = Sub_Sub_Category, y = Value_new, group = 1)) +
geom_line(color = "blue") +
geom_point(color = "red") +
theme_minimal() +
theme(
plot.background = element_rect(fill = "white", color = "white"),
panel.background = element_rect(fill = "white", color = "white"),
axis.text.x = element_text(angle = 45, hjust = 1)
) +
labs(title = "Time Series of Value_new",
x = "Sub_Sub_Category",
y = "Value_new") +
scale_x_discrete(expand = c(0, 0))
# plot을 PNG 형식으로 저장
ggsave(filename = file.path(path_to, paste0(output_file, ".png")), plot = p, bg = "white", width = 10, height = 6)
# 메시지 출력
cat("\n", crayon::magenta(message), "\n")
}
## 🟧 경로의 추출할 항목들 확인 ====================================================================================================
# 함수 정의
extract_unique_categories <- function(path) {
# 지정된 경로에서 파일 리스트 가져오기 (재귀적으로 하위 폴더 포함)
files <- list.files(path, full.names = TRUE, recursive = TRUE)
# 파일 이름만 추출
file_names <- basename(files)
# "연도_"와 첫 번째 "___" 사이의 문자열 추출
categories <- str_extract(file_names, "(?<=^[0-9]{4}_)[^_]+")
# 중복 제거 후 반환
unique_categories <- unique(categories)
return(unique_categories)
}
## 🟧 csv 파일들 처리 ====================================================================================================
library(dplyr)
process_csv_files <- function(path, colnames, which_year) {
# 파일 리스트 불러오기
files <- list.files(path, pattern = "\\.csv$", full.names = TRUE)
# 결과를 저장할 리스트
results <- list()
for (file in files) {
# CSV 파일 읽기
data <- read.csv(file)
# 열 이름 확인
col_names <- colnames(data)
# 3번째 열의 값 추출
third_column <- data[[3]]
# 연도만 추출 (정규표현식 사용)
years <- third_column[grep("\\d{4}", third_column)]
# which_year에 해당하는 행 추출
rows_with_year <- data[grep(which_year, third_column), ]
# colnames 내 모든 문자열을 포함하는 열 찾기 (4번째 열부터 Categorized_L3_New 이전까지)
start_col <- 4
end_col <- which(col_names == "Categorized_L3_New") - 1
if (length(end_col) == 0) {
stop("Categorized_L3_New 열이 존재하지 않습니다.")
}
# colnames 내 모든 문자열을 포함하는 열 찾기
matching_cols <- col_names[start_col:end_col]
target_cols <- matching_cols[sapply(matching_cols, function(x) all(sapply(colnames, function(y) grepl(y, x))))]
if (length(target_cols) == 0) {
warning(paste("파일", file, "에서 모든 colnames에 해당하는 열을 찾지 못했습니다."))
next
}
# 결과 저장
results[[file]] <- list(
rows_with_year = rows_with_year,
target_cols = target_cols
)
}
return(results)
}
## 🟧 연도 합계 파일이 아닌 파일들 삭제 ====================================================================================================
delete_non_year_files <- function(directory) {
# 모든 csv 파일을 재귀적으로 찾기
files <- list.files(directory, pattern = "\\.csv$", recursive = TRUE, full.names = TRUE)
# 연도 또는 연도를 포함한 문자열을 판단하기 위한 정규 표현식
year_regex <- "^[0-9]{4}.*$"
# 파일 삭제 카운터
deleted_files <- 0
for (file in files) {
# 파일 이름에서 확장자 제거
file_name <- basename(file)
file_name <- sub("\\.csv$", "", file_name)
# 파일 이름을 "___"로 분할
parts <- unlist(strsplit(file_name, "___"))
# 두 번째 부분이 연도 또는 연도를 포함한 문자열인지 확인
if (length(parts) >= 2) {
second_part <- parts[2]
if (!grepl(year_regex, second_part)) {
# 연도 또는 연도를 포함한 문자열이 아니면 파일 삭제
file.remove(file)
deleted_files <- deleted_files + 1
cat("Deleted:", file, "\n")
}
}
}
cat("Total files deleted:", deleted_files, "\n")
}
## 🟧 연도별 내보내고 최빈값으로 합치기 ====================================================================================================
each_year_total_copy_data_by_year_by_max_value_rows = function(yb,
path_from,
path_to,
L3,
item,
years_regions = c(),
total_include.list,
total_exclude.list,
message = T,
remove.files = F){
# path_to = path_to_upper
if(length(years_regions) > 0){
for(k in seq_along(total_include.list)){
# k=1
# k=i=1
# i=26
tryCatch({
cat(crayon::blue("\nStarting outer loop with k = "), k, "\n")
for(i in seq_along(years_regions)){
cat(crayon::blue(k, i, "\n"))
tryCatch({
cat(crayon::blue("  Starting inner loop with k = "), k, " and i = ", i, "\n")
ith_year = years_regions[i]
# change
kth_include.list = lapply(total_include.list[[k]], function(x){ c(x, ith_year) })
kth_exclude.list = total_exclude.list[[k]]
kth_sub_category = names(total_include.list)[k]
kth_path_to = file.path(path_to, kth_sub_category) # path 설정
# function
ith_results = copy_data_by_year(yb,
path_from,
path_to = kth_path_to,
save_file_name = paste0(L3, "___", kth_sub_category, "___", ith_year),
include.list = kth_include.list,
exclude.list = kth_exclude.list,
message,
remove.files)
cat(crayon::blue("  Finished inner loop with k = "), k, " and i = ", i, "\n")
}, error = function(e) {
cat(crayon::red("Error in inner loop with k = "), k, " and i = ", i, ": ", e$message, "\n")
stop(e)
})
}
# remove.files=F
tryCatch({
# 각 추출된 연도별 데이터 합치기 (최빈값 기준)
process_and_export_most_frequent_value_rows(path_to = kth_path_to, output_file = paste0(L3, "___", kth_sub_category, "___Total"))
cat(crayon::green("Exporting is done: "), crayon::bgMagenta(paste0(L3, "_", kth_sub_category)), "\n")
}, error = function(e) {
cat(crayon::red("Error in process_and_export_most_frequent_value_rows with k = "), k, ": ", e$message, "\n")
stop(e)
})
cat(crayon::blue("Finished outer loop with k = "), k, "\n")
}, error = function(e) {
cat(crayon::red("Error in outer loop with k = "), k, " and last i = ", if (exists("i")) i else "not started", ": ", e$message, "\n")
stop(e)
})
}
}else{
for(k in seq_along(total_include.list)){
# k=1
# k=i=1
# i=26
tryCatch({
cat(crayon::blue("\nStarting outer loop with k = "), k, "\n")
tryCatch({
# change
kth_include.list = total_include.list[[k]]
kth_exclude.list = total_exclude.list[[k]]
kth_sub_category = names(total_include.list)[k]
# kth_path_to = file.path(path_to, kth_sub_category) # path 설정
# function
kth_results = copy_data_by_year(yb,
path_from,
path_to = path_to,
save_file_name = paste0(kth_sub_category),
include.list = kth_include.list,
exclude.list = kth_exclude.list,
message,
remove.files)
}, error = function(e) {
cat(crayon::red("Error in inner loop with k = "), k, " and i = ", i, ": ", e$message, "\n")
stop(e)
})
# remove.files=F
# tryCatch({
#   # 각 추출된 연도별 데이터 합치기 (최빈값 기준)
#   process_and_export_most_frequent_value_rows(path_to = kth_path_to, output_file = paste0(L3, "___", kth_sub_category, "___Total"))
#
#   cat(crayon::green("Exporting is done: "), crayon::bgMagenta(paste0(L3, "_", kth_sub_category)), "\n")
# }, error = function(e) {
#   cat(crayon::red("Error in process_and_export_most_frequent_value_rows with k = "), k, ": ", e$message, "\n")
#   stop(e)
# })
cat(crayon::blue("Finished outer loop with k = "), k, "\n")
}, error = function(e) {
cat(crayon::red("Error in outer loop with k = "), k, " and last i = ", if (exists("i")) i else "not started", ": ", e$message, "\n")
stop(e)
})
}
}
}
# message=F
## 🟧 리스트 2개 비교 =============================================================
# 두 개의 리스트를 비교하여 길이와 원소 이름이 동일한지 확인하는 함수
# 두 개의 리스트를 비교하여 길이와 원소 이름이 동일한지 확인하는 함수
library(crayon)
# 두 개의 리스트를 비교하여 길이와 원소 이름이 동일한지 확인하는 함수
compare_lists <- function(list1, list2) {
# 두 리스트의 길이 비교
if (length(list1) != length(list2)) {
cat(red("The lists have different lengths.\n"))
return(FALSE)
}
# 두 리스트의 원소 이름 비교
names1 <- names(list1)
names2 <- names(list2)
if (is.null(names1) || is.null(names2)) {
cat(red("One or both of the lists do not have names.\n"))
return(FALSE)
}
if (!all(names1 == names2)) {
cat(red("The lists have different names.\n"))
return(FALSE)
}
cat(green("The lists have the same length and names.\n"))
return(TRUE)
}
## 🟧 csv 파일 필 ========================================================================
get_filtered_csv_files <- function(path, include = NULL, exclude = NULL) {
# Load necessary library
library(stringr)
# List all CSV files recursively
all_files <- list.files(path, pattern = "\\.csv$", recursive = TRUE, full.names = FALSE)
# Extract the specific part of the file name
extract_name_part <- function(filename) {
parts <- str_split(filename, "___")[[1]]
if (length(parts) >= 2) {
return(paste(parts[1], parts[2], sep = "___"))
} else {
return(NA)
}
}
extracted_names <- sapply(all_files, extract_name_part)
valid_files <- !is.na(extracted_names)
extracted_names <- extracted_names[valid_files]
all_files <- all_files[valid_files]
# Filter based on include criteria (all strings in include must be present)
if (!is.null(include)) {
include_indices <- sapply(include, function(inc) grepl(inc, extracted_names))
include_indices <- apply(include_indices, 1, all)
} else {
include_indices <- rep(TRUE, length(extracted_names))
}
# Filter based on exclude criteria (any string in exclude must not be present)
if (!is.null(exclude)) {
exclude_indices <- sapply(exclude, function(exc) grepl(exc, extracted_names))
exclude_indices <- apply(exclude_indices, 1, any)
} else {
exclude_indices <- rep(FALSE, length(extracted_names))
}
final_indices <- include_indices & !exclude_indices
filtered_files <- all_files[final_indices]
# Return only the names of the filtered files
return(basename(filtered_files))
}
# 🟥 데이터 로드 =================================================================
path_data_1 = "/Users/Ido/Documents/GitHub/KFS_Timeseries_Data/5.디지털숲가꾸기/숲가꾸기(2020_2024)_1.xlsx"
path_data_2 = "/Users/Ido/Documents/GitHub/KFS_Timeseries_Data/5.디지털숲가꾸기/조림(2020_2024).xlsx"
# 숲가꾸기 1 (2020 ~ 2022)
data_1 = read.xlsx(path_data_1)
View(data_1)
# 조림
data_2 = read.xlsx(path_data_2)
# names(data_1)
#
# View(data_1)
# 🌫️ data1   ==============================================================================================
## 🟪 check  =======================================================================================
names(data_1)
## 🟪 year col  =======================================================================================
colnames(data_1)[1] = "year"
data_1$year %>% unique
## 🟪 subset 2020, 2021  =======================================================================================
data_1$year %>% class
data_1_sub = data_1 %>% filter(year  %in% c("2020", "2021"))
dim(data_1_sub)
## 🟪 check the data  =======================================================================================
View(data_1_sub)
## 🟩 check the regions  =======================================================================================
data_1_sub$`GROUP_FIELD(필지)` %>% unique
## 🟩 Group regions  =======================================================================================
data_1_sub <- data_1_sub %>%
mutate(regions = case_when(
grepl("경기도", `GROUP_FIELD(필지)`) ~ "경기도",
grepl("경상남도", `GROUP_FIELD(필지)`) ~ "경상남도",
grepl("충청남도", `GROUP_FIELD(필지)`) ~ "충청남도",
grepl("전라남도", `GROUP_FIELD(필지)`) ~ "전라남도",
grepl("충청북도", `GROUP_FIELD(필지)`) ~ "충청북도",
grepl("경상북도", `GROUP_FIELD(필지)`) ~ "경상북도",
grepl("대전광역시", `GROUP_FIELD(필지)`) ~ "대전광역시",
grepl("전라북도", `GROUP_FIELD(필지)`) ~ "전라북도",
grepl("강원도", `GROUP_FIELD(필지)`) ~ "강원도",
grepl("강원특별자치도", `GROUP_FIELD(필지)`) ~ "강원도",
grepl("울산광역시", `GROUP_FIELD(필지)`) ~ "울산광역시",
grepl("서울특별시", `GROUP_FIELD(필지)`) ~ "서울특별시",
grepl("부산광역시", `GROUP_FIELD(필지)`) ~ "부산광역시",
grepl("인천광역시", `GROUP_FIELD(필지)`) ~ "인천광역시",
grepl("제주특별자치도", `GROUP_FIELD(필지)`) ~ "제주특별자치도",
grepl("대구광역시", `GROUP_FIELD(필지)`) ~ "대구광역시",
grepl("광주광역시", `GROUP_FIELD(필지)`) ~ "광주광역시",
grepl("세종특별자치시", `GROUP_FIELD(필지)`) ~ "세종특별자치시",
TRUE ~ "기타"
)) %>%
relocate(regions, .after = year)
# data_1_sub %>% filter(regi ons == "서울특별시") %>% View
# 결과 확인
head(data_1_sub)
data_1_sub$regions %>% table %>% as.data.frame()
# 기타?
data_1_sub %>% filter(regions == "기타") %>% View
## 🟨 필요열들 이름 변경  =======================================================================================
data_1_new = data_1_sub %>%
rename("work_area" = `PMS3A011_WORK_AREA(산림자원조성사업정보.작업면적)`) %>%
rename("forest_tending" = `PMS3A011_FRCMB_NM1(숲가꾸기내역.작업종1)`) %>%
relocate(work_area, forest_tending, .after = 2)
data_1_new$forest_tending %>% table
## 🟨 각 지역별 연도별 숲가꾸기 데이터 합산  =======================================================================================
# 각 지역별로 forest_tending에 따라 work_area를 합산한 새로운 데이터프레임 생성
data_aggregated <- data_1_new %>%
group_by(regions, forest_tending, year) %>%
summarise(total_work_area = sum(work_area, na.rm = TRUE)) %>%
ungroup()
# 결과 확인
data_aggregated %>% View
unique(data_1_new$regions) %in%  data_aggregated$regions
data_aggregated$regions %>% unique
# 검토
data_1_new %>%
filter(regions == "강원도" &
forest_tending == "어린나무가꾸기" &
year == "2020") %>%
pull(work_area) %>%
sum(na.rm = T)
data_aggregated %>%
filter(regions == "강원도" &
forest_tending == "어린나무가꾸기" &
year == "2020") %>%
pull(total_work_area)
## 🟨 연도별 재구성  =======================================================================================
data_aggregated_2021 = data_aggregated %>% filter(year == "2021")
data_aggregated_2020 = data_aggregated %>% filter(year == "2020")
data_aggregated$year
## 🟦 연보데이터 추출  =======================================================================================
### 🟧 데이터 로드 ===================================================================================
# 2021년도 데이터 -> 2022 연보를 의미
path_yb_2021 = "/Users/Ido/Documents/GitHub/KFS_Timeseries_Data/4.Exported Data_by ID_2/숲가꾸기/숲 가꾸기Forest tending/2022_YRBK_00520408.csv"
# 2020년도 데이터 -> 2021 연보를 의미
path_yb_2020 = "/Users/Ido/Documents/GitHub/KFS_Timeseries_Data/4.Exported Data_by ID_2/숲가꾸기/숲 가꾸기Forest tending/2021_YRBK_00510409.csv"
yb_2021 = read.csv(path_yb_2021) %>% relocate(year, .after = 3)
yb_2020 = read.csv(path_yb_2020) %>% relocate(year, .after = 3)
View(yb_2021)
View(yb_2020)
### 🟧 Check names ===================================================================================
# names(yb_2021)
names(yb_2020)[3] = names(yb_2021)[3] = "classification"
names(yb_2020)[6] = names(yb_2021)[6] = "조림지가꾸기"
names(yb_2020)[9] = names(yb_2021)[9] = "어린나무가꾸기"
names(yb_2020)[10] = names(yb_2021)[10] = "큰나무가꾸기"
### 🟧 Extract data ===================================================================================
View(yb_2021)
View(yb_2021)
yb_2021_sub = yb_2021 %>%
select(classification, 조림지가꾸기, 어린나무가꾸기, 큰나무가꾸기) %>%
filter(classification %in% data_aggregated$regions)
yb_2020_sub = yb_2020 %>%
select(classification, 조림지가꾸기, 어린나무가꾸기, 큰나무가꾸기) %>%
filter(classification %in% data_aggregated$regions)
### 🟧 데이터재구성 ===================================================================================
yb_2021_sub %>% head
names(yb_2021_sub)
# 데이터 변환 함수
transform_data <- function(data, year_value) {
data %>%
pivot_longer(cols = c("조림지가꾸기", "어린나무가꾸기", "큰나무가꾸기"),
names_to = "forest_tending",
values_to = "total_work_area") %>%
rename(regions = classification) %>%
select(regions, forest_tending, total_work_area)
}
yb_2021_sub_2 = transform_data(yb_2021_sub)
yb_2020_sub_2 = transform_data(yb_2020_sub)
View(yb_2021_sub_2)
yb_2021_sub_2 %>% filter(regions == "서울특별시")
# 🟥 데이터 합치기  ===================================================================================
# 데이터 체크
data_aggregated_2021
data_aggregated_2020
yb_2021_sub_2
yb_2020_sub_2
View(yb_2021)
names(data_aggregated_2021)
names(yb_2021_sub_2)
yb_2021_sub_2$forest_tending %>% table
data_aggregated_2021 %>% filter(regions == "서울특별시")
yb_2021 %>% View
# 두 데이터프레임 병합 (regions와 forest_tending을 기준으로)
combined_data_2021 <- left_join(data_aggregated_2021, yb_2021_sub_2,
by = c("regions", "forest_tending"),
suffix = c("_digital", "_yb"))
combined_data_2020 <- left_join(data_aggregated_2020, yb_2020_sub_2,
by = c("regions", "forest_tending"),
suffix = c("_digital", "_yb"))
# 🟥 ha로  unit 바꾸기  ===================================================================================
combined_data_2021 = combined_data_2021 %>%
mutate(total_work_area_digital_ha = total_work_area_digital / 10000) %>%
mutate(diff_abs = abs(total_work_area_digital_ha - total_work_area_yb))
combined_data_2020 = combined_data_2020 %>%
mutate(total_work_area_digital_ha = total_work_area_digital / 10000) %>%
mutate(diff_abs = abs(total_work_area_digital_ha - total_work_area_yb))
View(combined_data_2020)
# 🟥 export  ===================================================================================
path_save = "/Users/Ido/Documents/GitHub/KFS_Timeseries_Data/5.디지털숲가꾸기/Exported_2"
write.csv(combined_data_2020, file.path(path_save, "comparison 2020.csv"), row.names = F)
write.csv(combined_data_2021, file.path(path_save, "comparison 2021.csv"), row.names = F)
# 🟥 Load Functions & Packages ##########################################################################
Sys.setlocale("LC_ALL", "en_US.UTF-8")
## 🟨Install and loading Packages ================================
# rm(list = ls())
install_packages = function(packages, load=TRUE) {
# load : load the packages after installation?
for(pkg in packages) {
if (!require(pkg, character.only = TRUE)) {
install.packages(pkg)
}
if(load){
library(pkg, character.only = TRUE, quietly = T)
}
}
}
List.list = list()
List.list[[1]] = visual = c("ggpubr", "ggplot2", "ggstatsplot", "ggsignif", "rlang", "RColorBrewer")
List.list[[2]] = stat = c("fda", "MASS")
List.list[[3]] = data_handling = c("tidyverse", "dplyr", "clipr", "tidyr", "readr", "caret", "readxl")
List.list[[4]] = qmd = c("janitor", "knitr")
List.list[[5]] = texts = c("stringr", "stringi")
List.list[[6]] = misc = c("devtools")
List.list[[7]] = db = c("RMySQL", "DBI", "odbc", "RSQL", "RSQLite")
List.list[[8]] = sampling = c("rsample")
List.list[[9]] = excel = c("openxlsx")
List.list[[10]] = others = c("beepr")
packages_to_install_and_load = unlist(List.list)
install_packages(packages_to_install_and_load)
## 🟧dplyr ==================================================================================================
filter = dplyr::filter
select = dplyr::select
# 🟥 =========================================================================================================
list.files("/Volumes/ADNI_SB_SSD_NTFS_4TB_Sandisk/New/Completed_GE.MEDICAL.SYSTEMS/GE.MEDICAL.SYSTEMS_SB___Sub_001___RID_0074___EPB_I1120325___MT1_I1120324/Results")
